# Non-Agentic AI Contract  
**v0.1 — Draft for Pilot Service Line**

---

## 1. Purpose

This contract defines the **mandatory boundaries** under which artificial intelligence may be used within Syner-G’s AI service line in regulated, phase-gated environments.

Its purpose is to ensure that:
- AI **supports** human decision-making,
- AI **never replaces** human authority,
- all AI assistance remains **auditable, explainable, and defensible**, and
- regulatory trust is **preserved or strengthened**, never eroded.

This document is intentionally conservative. Speed, automation, and novelty are subordinate to **decision defensibility**.

---

## 2. Scope

This contract applies to:

- All AI-assisted work delivered through Syner-G’s AI service line  
- All internal prototypes, pilots, and client-facing solutions involving:
  - LLMs
  - machine learning models
  - generative or analytical AI
- All regulated or quasi-regulated contexts, including but not limited to:
  - non-clinical informatics
  - IND-enabling activities
  - CMC and regulatory documentation support
  - phase-gate decision support

This contract governs **how AI may assist**, not **what conclusions are reached**.

---

## 3. Foundational Principle

> **AI may prepare information for a decision.  
> Only humans may make, approve, or own decisions.**

No exception.

---

## 4. Explicit Non-Agentic Stance

Under this contract:

- AI systems **do not initiate decisions**
- AI systems **do not approve decisions**
- AI systems **do not act autonomously**
- AI systems **do not execute downstream actions**
- AI systems **do not replace accountable roles**

AI outputs are **inputs**, never outcomes.

---

## 5. Permitted AI Functions (Bounded Assistance)

AI may be used to:

- Surface inconsistencies across documents or data sources
- Highlight missing inputs, dependencies, or assumptions
- Summarize large volumes of material for human review
- Compare current artifacts against historical or reference patterns
- Flag potential risk signals or contradictions
- Normalize information into structured, reviewable formats
- Support scenario framing (“if / then” conditions)

All outputs must be:
- reviewable,
- challengeable,
- discardable without penalty.

---

## 6. Hard Prohibitions (Non-Negotiable)

AI **must never**:

- Render go / no-go determinations
- Approve or reject regulatory submissions
- Modify source data or authoritative records
- Override documented human decisions
- Mask uncertainty or fabricate justification
- Create undocumented changes to scope, risk, or evidence
- Be positioned as a decision-maker to clients or regulators

Any system that violates these prohibitions is **out of scope** for the service line.

---

## 7. Human Authority Guarantees

For every AI-assisted output:

- A named human owner is required
- That owner explicitly affirms or rejects the output
- Responsibility remains with the human, not the system
- Silence or inaction **never** implies acceptance

No AI output is considered valid until a human accepts responsibility for it.

---

## 8. Transparency & Explainability Requirements

All AI-assisted outputs must:

- Clearly indicate that AI assistance was used
- Identify the role of AI in producing the output
- Preserve traceability to source inputs
- Avoid anthropomorphic language (“the model decided…”)

If an output cannot be explained to a reviewer, it **cannot be used**.

---

## 9. Auditability & Recordkeeping

AI assistance must produce an audit trail that includes:

- Input sources used
- Output generated
- Human reviewer identity
- Acceptance, modification, or rejection
- Timestamped decision context

These records must be:
- retained according to project governance rules
- available for internal and external review
- immutable once finalized

---

## 10. Escalation & Failure Handling

If AI output:

- conflicts with expert judgment,
- introduces ambiguity in a regulated decision,
- surfaces unexpected risk,
- or behaves outside expected bounds,

the system must:
1. halt progression,
2. surface the issue explicitly,
3. require human review before continuation.

AI systems **may not silently fail**.

---

## 11. RGDS Integration Clause

When used alongside Regulated Gate Decision Support (RGDS):

- AI outputs may populate **decision context fields**
- AI may assist in evidence synthesis and risk surfacing
- AI **may not** populate final decision fields
- All AI contributions must be explicitly marked within the decision log
- RGDS remains the authoritative decision record

RGDS decisions are **human-governed by design**; AI assistance does not alter this.

---

## 12. Change Control

This contract is versioned and controlled.

- Revisions require:
  - documented rationale
  - review by service-line leadership
  - explicit version increment
- No silent changes
- No local overrides

Stability is a feature.

---

## 13. Intentional Conservatism

This contract prioritizes:
- trust over speed
- defensibility over novelty
- clarity over capability

As the service line matures, boundaries may evolve — but **only deliberately, never implicitly**.

---

### Status

**v0.1 — Draft**  
Intended for pilot use, internal alignment, and controlled client engagement.
